


from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from scipy.stats import randint
from util.custom_estimators import CombinedAtribbutesAdder
import joblib
import os
import pandas as pd

HOUSING_PATH = os.path.join('..', 'datasets', 'housing')  

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, 'housing.csv')
    return pd.read_csv(csv_path)





housing = load_housing_data()
housing.head()


train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

housing = train_set.drop("median_house_value", axis=1).copy()
housing_labels = train_set["median_house_value"].copy()

housing_num = housing.drop("ocean_proximity", axis=1)
num_attribs = list(housing_num.columns)
cat_attribs = ["ocean_proximity"]

pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("new_attribs", CombinedAtribbutesAdder()),
    ("scale", StandardScaler()),
])

trans_pipe = ColumnTransformer([
    ("num", pipeline, num_attribs),
    ("cat", OneHotEncoder(), cat_attribs)
])

housing_prepared = trans_pipe.fit_transform(housing)
pd.DataFrame(housing_prepared).head()





# ran_for = RandomForestRegressor()
#
# distribution = dict(max_features=randint(1, 9),
#                    n_estimators=randint(3, 100))
# 
# random_search = RandomizedSearchCV(ran_for, distribution, n_iter=50,
#                                    scoring="neg_mean_squared_error")
#
# random_search.fit(housing_prepared, housing_labels)
# joblib.dump(random_search, "models/ran_search_ran_for")


random_search = joblib.load("models/ran_search_ran_for")


random_search.best_estimator_


random_search.best_params_


random_search.cv_results_
